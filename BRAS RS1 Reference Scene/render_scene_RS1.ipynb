{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# BRAS Reference Scene RS1\n",
    "\n",
    "This Notebook reproduces the BRAS Reference Scene RS1 with Mitsuba3.\n",
    "\n",
    "A speaker and a microphone are positioned above an absorber panel which is flat on a fully reflecting concrete floor. \n",
    "\n",
    "\n",
    "<div align='center'>\n",
    "<img src='1 Scene descriptions/RS1 single reflection (infinite plate)/Geometry/RS1_RIR_Absorbing.png' alt='Alt text' style='width:70%;'>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import drjit as dr\n",
    "from platform import system\n",
    "import pyfar as pf\n",
    "import sofar as sf\n",
    "import mitsuba as mi\n",
    "\n",
    "if system() == 'Darwin':\n",
    "    mi.set_variant('llvm_ad_rgb')\n",
    "else:\n",
    "    mi.set_variant('cuda_ad_rgb')\n",
    "print(f'Mitsuba variant: {mi.variant()}')\n",
    "\n",
    "from mitsuba import ScalarTransform4f as T\n",
    "\n",
    "\n",
    "# set retina backend for plotting with matplotlib magic\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "mi.set_log_level(mi.LogLevel.Warn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microphone and speaker positions\n",
    "\n",
    "The Room Impulse Responses (RIRs) are stored in the measurement SOFA-file. The SOFA-file contains the positions of the emitter and receiver as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sofafile = sf.read_sofa('1 Scene descriptions/RS1 single reflection (infinite plate)/RIRs/RS1_RIRs_Absorbing.sofa')\n",
    "# sofafile.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DATA is stored in 9 channels for the 9 combinations of Emitter and Receiver positions. We can set the receiver and emitter ID and then select the corresponding channel of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EmitterID = 2\n",
    "ReceiverID = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Shape of the RIR Data: {sofafile.Data_IR.shape}')\n",
    "\n",
    "# find sofafile index by comparing mic_ID and speaker_ID to sofafile.EmitterID and sofafile.ReceiverID\n",
    "sofafile_index = np.where((sofafile.EmitterID == EmitterID) & (sofafile.ReceiverID == ReceiverID))[0][0]\n",
    "\n",
    "print(f'Using sofafile index {sofafile_index} for Mic ID {EmitterID} and Speaker ID {ReceiverID}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_pos = sofafile.ReceiverPosition[0, :, sofafile_index]\n",
    "emitter_pos = sofafile.EmitterPosition[0, :, sofafile_index]\n",
    "reflection_pos = np.array([5.500, 2.985, 0.020]) # the reflection position is designed to be at the center of the absorber\n",
    "\n",
    "# in the sofa files, z=0 is at the height of the absorber. In the Mitsuba scene, z=0 is at floor height.\n",
    "# therefore, we need to shift the z coordinate by the absorber height (20 mm)\n",
    "mic_pos[2] += 0.02\n",
    "emitter_pos[2] += 0.02\n",
    "\n",
    "\n",
    "print(f'\\nEmitter ID: {EmitterID}, Receiver ID: {ReceiverID}')\n",
    "print(f'Emitter position:\\t{emitter_pos}')\n",
    "print(f'Receiver position:\\t{mic_pos}')\n",
    "print(f'Reflection position:\\t{reflection_pos}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Rendering of the Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_visual = {\n",
    "    'mic_pos':        mic_pos,\n",
    "    'speaker_pos':    emitter_pos,\n",
    "    'reflection_pos': reflection_pos,\n",
    "\n",
    "    'camera_pos': [5.5, -6, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary that we will use to build the scene, all objects, materials and parameters are defined in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_dict_visual = {\n",
    "    'type': 'scene',\n",
    "\n",
    "    'integrator': {\n",
    "        'type': 'path',\n",
    "        'max_depth': 3,\n",
    "    },\n",
    "\n",
    "    'sensor': {\n",
    "        'type': 'perspective',\n",
    "        'to_world': T.look_at(\n",
    "            origin=config_visual['camera_pos'],\n",
    "            target=config_visual['reflection_pos'],\n",
    "            up=[0, 0, 1]\n",
    "        ),\n",
    "        'fov': 65,\n",
    "        'film': {\n",
    "            'type': 'hdrfilm',\n",
    "            'width': 1920,\n",
    "            'height': 1080,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'speaker': {\n",
    "        'type': 'cube',\n",
    "        'to_world': T.look_at(\n",
    "            origin=config_visual['speaker_pos'],\n",
    "            target=config_visual['reflection_pos'],\n",
    "            up=[1, 0, 0]\n",
    "        ).translate((0, 0, -0.2)).scale((0.2, 0.3, 0.2)),\n",
    "        'material': {\n",
    "            'type': 'diffuse',\n",
    "            'reflectance': 0.3\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'microphone': {\n",
    "        'type': 'cylinder',\n",
    "        'to_world': T.look_at(\n",
    "            origin=config_visual['mic_pos'],\n",
    "            target=config_visual['reflection_pos'],\n",
    "            up=[0, 0, 1]\n",
    "        ).translate((0, 0, -0.2)).scale((0.05, 0.05, 0.2)),\n",
    "        'material': {\n",
    "            'type': 'diffuse',\n",
    "            'reflectance': {\n",
    "                'type': 'rgb',\n",
    "                'value': (0, 1, 0)\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'microphone_sphere': {\n",
    "        'type': 'sphere',\n",
    "        'radius': 0.05,\n",
    "        'center': config_visual['mic_pos'],\n",
    "        'material': {\n",
    "            'type': 'diffuse',\n",
    "            'reflectance': {\n",
    "                'type': 'rgb',\n",
    "                'value': (0, 1, 0)\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'concrete': {\n",
    "        'type': 'rectangle',\n",
    "        'to_world': T.translate(config_visual['reflection_pos']).scale((5.5, 2.985, 1)),\n",
    "        'material': {\n",
    "            'type': 'plastic',\n",
    "            'int_ior': 1.9,\n",
    "            'diffuse_reflectance': {\n",
    "                'type': 'rgb',\n",
    "                'value': np.array((53,33,20))/256\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'absorber': {\n",
    "        'type': 'rectangle',\n",
    "        'to_world': T.translate(config_visual['reflection_pos']).translate((0, 0, 0.02)).scale((5.5-3.397, 2.985-0.881, 1)),\n",
    "        'material': {\n",
    "            'type': 'diffuse',\n",
    "            'reflectance': {\n",
    "                'type': 'rgb',\n",
    "                'value': np.array((254/256, 250/256, 241/256))\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'ceiling_light': {\n",
    "        'type': 'rectangle',\n",
    "        'flip_normals': True,\n",
    "        'to_world': T.translate(config_visual['reflection_pos']).translate((0, 0, 10)).scale((.8, .7, 1)),\n",
    "        'emitter': {\n",
    "            'type': 'area',\n",
    "            'radiance': {\n",
    "                'type': 'rgb',\n",
    "                'value': 200.0,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'background_floor': {\n",
    "        'type': 'rectangle',\n",
    "        'to_world': T.translate((0, 0, -0.001)).scale((30, 30, 1)),\n",
    "        'material': {\n",
    "            'type': 'diffuse',\n",
    "            'reflectance': {\n",
    "                'type': 'checkerboard',\n",
    "                'to_uv': T.scale([30, 30, 1])\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the visual scene.\n",
    "\n",
    "If the rendering process takes a long time, reduce the samples per pixel `spp`. The renderer produces a noisy but still usable image even with `spp=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = mi.load_dict(scene_dict_visual)\n",
    "\n",
    "img = mi.render(scene, spp=50)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.title(f'Microphone position {EmitterID}, Speaker position {ReceiverID}')\n",
    "plt.imshow(img** (1.0 / 2.2))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Material Parameters\n",
    "\n",
    "The files `mat_RockFonSonarG_00deg.csv` and `mat_CR1_concrete.csv` contain data for the absorption and scattering coefficients of the concrete flooring and the absorber panels.\n",
    "\n",
    "We want to store the data in separate lists. Also, to reduce rendering time, we only want to use the frequencies specified in `frequencies`.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<strong>Info:</strong> This way of storing the absorption data will be deprecated once full spectral acoustic rendering is implemented.\n",
    "\n",
    "Once implemented, the acoustic variant will work analoguously to the spectral variant and the `tape` plugin will work analoguously to the `specfilm` plugin, where the absorption and scattering spectra will need to be loaded as `acoustic` spectrum types and the tape plugin will have channels with channel-specific sensitivity spectra, e.g. octave bands or third octave bands. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_materials = '3 Surface descriptions/_csv/initial_estimates'\n",
    "\n",
    "path.join(path_materials, 'mat_RockFonSonarG_00deg.csv')\n",
    "absorber = np.genfromtxt(path.join(path_materials, 'mat_RockFonSonarG_00deg.csv'), delimiter=',')\n",
    "concrete = np.genfromtxt(path.join(path_materials, 'mat_CR1_concrete.csv'), delimiter=',')\n",
    "\n",
    "# Reduce the size of the absorption data to the frequencies specified below\n",
    "frequencies = np.array([125, 16000])\n",
    "# frequencies = absorber[0] # uncomment this line to render all frequencies instead.\n",
    "\n",
    "absorption_absorber = absorber[1, np.isin(absorber[0], frequencies)]\n",
    "scattering_absorber = absorber[2, np.isin(absorber[0], frequencies)]\n",
    "\n",
    "absorption_concrete = concrete[1, np.isin(concrete[0], frequencies)]\n",
    "scattering_concrete = concrete[2, np.isin(concrete[0], frequencies)]\n",
    "\n",
    "print(f'Frequencies:\\t\\t{frequencies}')\n",
    "print(f'Absorption absorber:\\t{absorption_absorber}')\n",
    "print(f'Scattering absorber:\\t{scattering_absorber}')\n",
    "print(f'Absorption concrete:\\t{absorption_concrete}')\n",
    "print(f'Scattering concrete:\\t{scattering_concrete}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acoustic Rendering\n",
    "\n",
    "First, set the variant to an acoustic variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if system() == 'Darwin':\n",
    "    mi.set_variant('llvm_ad_acoustic')\n",
    "else:\n",
    "    mi.set_variant('cuda_ad_acoustic')\n",
    "print(f'Mitsuba variant: {mi.variant()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important scene parameters like the speed of sound, absorption and scattering spectra, as well as the number of time bins and the maximum time are set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_acoustics = {\n",
    "    'speed_of_sound': 343.0,\n",
    "    'speaker_radius': 0.1,\n",
    "\n",
    "    'absorption_concrete': [(i + 1, a) for i, a in enumerate(absorption_concrete)],\n",
    "    'scattering_concrete': [(i + 1, s) for i, s in enumerate(scattering_concrete)],\n",
    "\n",
    "    'absorption_absorber': [(i + 1, a) for i, a in enumerate(absorption_absorber)],\n",
    "    'scattering_absorber': [(i + 1, s) for i, s in enumerate(scattering_absorber)],\n",
    "\n",
    "    'integrator': 'prb_acoustic',\n",
    "    'wav_bins': len(absorption_concrete),   # x\n",
    "    'max_depth': 2,                         # maximum number of bounces, 2 is enough because there is only one reflection\n",
    "    'max_time':  0.025,\n",
    "    'time_bins': 750,                       # y\n",
    "    'spp': 2**22,                           # samples per time bin\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary that we will use to build the acoustic scene, all objects, materials and parameters are defined in here.\n",
    "\n",
    "The acoustic scene uses an acoustic integrator, the acoustic BSDFs and the actual speaker and microphone as emitter and sensor, respectively. Otherwise, the scene definition is the same as the visual scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_dict_acoustic = {\n",
    "    'type': 'scene',\n",
    "\n",
    "    'integrator': {\n",
    "        'type': config_acoustics['integrator'],\n",
    "        'max_depth': config_acoustics['max_depth'],\n",
    "        'max_time': config_acoustics['max_time'],\n",
    "        'speed_of_sound': config_acoustics['speed_of_sound'],\n",
    "        'skip_direct': False,\n",
    "    },\n",
    "\n",
    "    'microphone': {\n",
    "        'type': 'microphone',\n",
    "        'to_world': T.translate(mic_pos),\n",
    "        'film': {\n",
    "            'type': 'tape',\n",
    "            'wav_bins':  config_acoustics['wav_bins'],\n",
    "            'time_bins': config_acoustics['time_bins'],\n",
    "            'rfilter': {'type': 'box'},\n",
    "            'count': True\n",
    "        },\n",
    "        'sampler': {'type': 'stratified', 'sample_count': config_acoustics['spp']},\n",
    "    },\n",
    "\n",
    "    'speaker': {\n",
    "        'type': 'sphere',\n",
    "        'radius': config_acoustics['speaker_radius'],\n",
    "        'center': emitter_pos,\n",
    "        'emitter': {'type': 'area', 'radiance': {'type': 'uniform', 'value': 100.}},\n",
    "\n",
    "    },\n",
    "\n",
    "    'absorber': {\n",
    "        'type': 'rectangle',\n",
    "        'to_world': T.translate(config_visual['reflection_pos']).translate((0, 0, 0.02)).scale((5.5-3.397, 2.985-0.881, 1)),\n",
    "        'material': {\n",
    "            'type': 'acousticbsdf',\n",
    "            'absorption': {\n",
    "                'type': 'spectrum',\n",
    "                'value': config_acoustics['absorption_absorber'],\n",
    "            },\n",
    "            'scattering': {\n",
    "                'type': 'spectrum',\n",
    "                'value': config_acoustics['scattering_absorber'],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'concrete': {\n",
    "        'type': 'rectangle',\n",
    "        'to_world': T.translate(config_visual['reflection_pos']).scale((5.5, 2.985, 1)),\n",
    "        'material': {\n",
    "            'type': 'acousticbsdf',\n",
    "            'absorption': {\n",
    "                'type': 'spectrum',\n",
    "                'value': config_acoustics['absorption_concrete'],\n",
    "            },\n",
    "            'scattering': {\n",
    "                'type': 'spectrum',\n",
    "                'value': config_acoustics['scattering_concrete'],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "}\n",
    "scene = mi.load_dict(scene_dict_acoustic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the scene. Note that for acoustic path tracing we need to set `spp` bin a lot higher than in the visual rendering to get good results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(mi.render(scene, seed=0, spp=config_acoustics['spp']))\n",
    "hist = data[:, :, 0] / config_acoustics[\"spp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that our renderer produces the correct results we can calculate the time of arrival (TOA) of the direct sound and the floor reflection by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected TOA of direct sound:\n",
    "TOA_direct = np.linalg.norm(emitter_pos - mic_pos)  / config_acoustics['speed_of_sound']\n",
    "\n",
    "# Expected TOA of reflected sound:\n",
    "TOA_reflected = (np.linalg.norm(emitter_pos - reflection_pos) + np.linalg.norm(reflection_pos - mic_pos) ) / config_acoustics['speed_of_sound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(0., config_acoustics['max_time'], config_acoustics['time_bins'], endpoint=True)\n",
    "\n",
    "plt.plot(time, hist, label=frequencies)\n",
    "plt.ylabel('Energy')\n",
    "plt.xlabel('Time in ms')\n",
    "plt.vlines(TOA_direct, -np.max(hist)*0.1, 0, colors='r', linestyles='dashed', label='TOA direct')\n",
    "plt.vlines(TOA_reflected, -np.max(hist)*0.1, 0, colors='g', linestyles='dashed', label='TOA reflected')\n",
    "plt.ylim(-np.max(hist)*0.1, np.max(hist)*1.1)\n",
    "\n",
    "plt.legend(title='Frequency in Hz')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the peaks of the direct sound and reflected sound appear just before the estimated TOAs. This is not a misaligned time axis, nor is this an artifact of unsufficient time bin resolution. \n",
    "\n",
    "This behavior is expected because the speaker is not modeled as a point source but as a sphere with non-zero radius. \n",
    "\n",
    "The estimated TOAs shown in the dashed lines are the time it takes the sound to propagate from the _center_ of the speaker to the microphone. The histogram data shows the time it took the rays to propagate from the microphone to the _surface_ of the sphere.\n",
    "\n",
    "The difference between the point source distance and the actual ray lengths can range from 0 (the ray hits the speaker sphere tangentially) to the speaker radius (the ray hits the speaker parallel to the sphere's normal vector). The following plot shows that the histogram peaks correspond to this time range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_correct = config_acoustics['speaker_radius']/config_acoustics['speed_of_sound']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(time, hist, label=frequencies)\n",
    "plt.ylabel('Energy')\n",
    "plt.xlabel('Time in s')\n",
    "plt.vlines(TOA_direct, -np.max(hist)*0.1, 0, colors='r', linestyles='dashed', label='direct sound')\n",
    "plt.vlines(TOA_direct-t_correct, -np.max(hist)*0.1, 0, colors='r', linestyles='dashed',)\n",
    "plt.vlines(TOA_reflected, -np.max(hist)*0.1, 0, colors='g', linestyles='dashed', label='reflection')\n",
    "plt.vlines(TOA_reflected-t_correct, -np.max(hist)*0.1, 0, colors='g', linestyles='dashed')\n",
    "plt.ylim(-np.max(hist)*0.1, np.max(hist)*1.1)\n",
    "plt.xlim(0, 0.025)\n",
    "plt.title('TOA estimation with non-zero speaker radius')\n",
    "plt.legend(title='Frequency in Hz')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to measured RIR\n",
    "\n",
    "To compare the simulation to the measured data, we can select the correct Impulse Response from the SOFA file with the `sofafile_index` we used to select the correct Emitter and Receiver positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRs, _, _ = pf.io.read_sofa('1 Scene descriptions/RS1 single reflection (infinite plate)/RIRs/RS1_RIRs_Absorbing.sofa')\n",
    "impulse_response = IRs[sofafile_index]\n",
    "\n",
    "print(f'\\nIRs:\\n{IRs}')\n",
    "print(f'impulse_response:\\n{impulse_response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the impulse response in the time domain with the `pyfar.plot.time` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1)\n",
    "pf.plot.time(impulse_response, unit='s', ax=ax[0], dB=False)\n",
    "pf.plot.time(impulse_response, unit='s', ax=ax[1], dB=True)\n",
    "ax[0].set_xlim(0, 0.025)\n",
    "ax[1].set_xlim(0, 0.025)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the squared impulse response to the energy histogram. We can average the simulated data across the simulated frequencies to get the broadband histogram.\n",
    "\n",
    "Apart from the time shift we introduced with the non-zero emitter radius (discussed earlier), the results of the simulation match the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_avg = np.mean(hist, axis=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(impulse_response.times, impulse_response.time[0,:]**2/np.max(impulse_response.time[0,:]**2), label='Measured')\n",
    "plt.plot(time, hist_avg/np.max(hist_avg), label=f'Simulated')\n",
    "\n",
    "plt.vlines(TOA_direct, -0.05, 0, colors='r', linestyles='dashed', label='TOA direct')\n",
    "plt.vlines(TOA_reflected, -0.05, 0, colors='g', linestyles='dashed', label='TOA reflected')\n",
    "\n",
    "plt.xlim(0, 0.025)\n",
    "plt.ylim(-0.05, 1.1)\n",
    "plt.xlabel('Time in s')\n",
    "plt.ylabel(r'$E/E_\\mathrm{max}$')\n",
    "plt.title('Comparison of measured and simulated energy histograms')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitsubadev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
